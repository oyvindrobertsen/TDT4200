%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Short Sectioned Assignment
% LaTeX Template
% Version 1.0 (5/5/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage[utf8]{inputenc}

\usepackage{graphicx}

\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template

\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{	
\normalfont \normalsize 
\textsc{TDT4200 - Parallell Computing, IDI, NTNU} \\ [25pt] % Your university, school and/or department name(s)
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge Problem set 2 - Theory \\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{Ã˜yvind Robertsen} % Your name

\date{\normalsize\today} % Today's date or a custom date

\begin{document}

\maketitle % Print the title

%----------------------------------------------------------------------------------------
%	PROBLEM 1
%----------------------------------------------------------------------------------------

\section{General Theory}

\subsection{Architectures \& Programming Models}

\subsubsection{Differences between some architectures}

\begin{itemize}
    \item \textbf{Nvidia Maxwell} - Maxwell is a GPU microarchitecture composed of many Streaming Multiprocessors (SMs).
        Each of these contain 128 homogenous cores.
        SMs have some local memory that is shared between cores, but each SM has uniform access time to the central shared memory of the GPU, making Maxwell a non-NUMA architecture.
        Maxwell is made to support the CUDA platform, allowing for a large number of threads to be executed in parallell across cores.
    \item \textbf{ARM big.LITTLE} - big.LITTLE is a computing architecture using heterogenous cores to adjust dynamically to current needs.
        Typically, slower, low-power cores are coupled with big, powerful and power-hungry cores to allow for both sustained, low-intensity computation as well as supporting bursts of high-intensity computation.
        big.LITTLE is another non-NUMA architecture.
        In the architecture, the cores are arranged into clusters of either "big" cores or "LITTLE" cores.
        The scheduler does not see the individual cores, but rather only these clusters, and makes the decision of which one to utilize.
        If at least one "big" core is needed, that cluster is activated, otherwise the "LITTLE" cluster is used by default.
        One the most powerful ways of using the big.LITTLE, is by prioritizing threads so that high-priority/computationally intensive threads are executed on the "big" cluster, while low-priority/low computational intensity threads are performed on the "LITTLE" cluster.
    \item \textbf{Vilje} - Vilje is the supercomputer at NTNU.
        It consists of 1404 nodes across 19.5 racks, each node consisting of two eight-core processors.
        Each processor is coupled with a 16GB NUMA node, resulting in 32GB memory per node.
        Vilje is, in other words, a homogenous NUMA architecture.
        Vilje can be seen as a tightly coupled cluster of computing nodes.
        
    \item \textbf{Modern-day CPU} - A typical modern-day CPU has multiple homogenous cores, is non-NUMA (uniform access to main memory) and is not a cluster.
        They are also often developed with multithreaded execution in mind.
        
\end{itemize}

\subsubsection{Nvidia SIMT}

Nvidia's SIMT utilizes the cores in a Streaming Multiprocessor to execute threads.
Groups of 32 cores execute the same instruction at the same time, on different data.
In this regard, SIMT is similar to SIMD and vector computing.
SIMT can however run different threads in parallell, thus making it more flexible than traditional SIMD architectures.
SIMT can be seen as a hybrid between SIMD and MIMD, executing instructions in a thread synchronously across the cores used for that thread (SIMD), but retaining the ability to run different instruction streams in different threads (MIMD).
Of the architectures listed above, Nvidia Maxwell is the only one to support SIMT.

\subsubsection{Classification wrt. Flynn's Taxonomy}

\begin{itemize}
    \item \textbf{Nvidia Maxwell} - SIMD/MIMD hybrid, SIMT.
    \item \textbf{ARM big.LITTLE} - MIMD - can execute multiple instruction streams on different datasets.
    \item \textbf{Vilje} - MIMD - is comprised of several computing nodes that use modern multicore processors, each supporting MIMD on their own. This makes Vilje it self an MIMD architecture.
    \item \textbf{Modern CPU} - Modern CPUs are often multicore, allowing for MIMD.
\end{itemize}

\subsection{CUDA GPGPUs}

\subsubsection{Terminology}

\textbf{Thread} - Instruction stream with it's own register values, but sharing memory with other threads.

\textbf{Block} - Grouping of threads. Each block executes on an SM.

\textbf{Grid} - Grouping of blocks.

\subsubsection{Deciding n}

We begin by finding $t_{CPU}$ and $t_{GPU}$.

\begin{gather*}
    t_{CPU} = 3500n\log_{2} n \\
    t_{GPU} = \frac{7n}{r} + 35n\log_{2} n
\end{gather*}

We find an expression for n by setting $t_{CPU} = t_{GPU}$

\begin{gather*}
    t_{CPU} = t_{GPU} \\
    \downarrow \\
    n = 2^{\frac{1}{495r}}
\end{gather*}

For any value of $n$ larger than $\frac{1}{495r}$, the GPU version will be faster.

\subsubsection{Fastest kernel}

\texttt{kernel2} will execute the fastest, as it calls \texttt{some\_other\_task} only every eight blocks instead of every eight threads.

\subsubsection{More terms}

\begin{itemize}
    \item \textbf{Warps} - Grouping of threads that are scheduled to run simultaneously on an SM.
    \item \textbf{Occupancy} - To which degree a warp is "full". Each warp can contain a maximum number of threads. To maximize performance, we want to fill each warp as much as possible. An almost empty warp will be scheduled on an SM, and leave most of the cores idle.
    \item \textbf{Memory Coalescing} - grouping of data in memory or grouping of data transfers. Performing memory coalesing when accessing global memory from a kernel is important to maximize performance.
    \item \textbf{Local Memory} - An area in main memory that is allocated to a thread. Used when all registers are in used.
    \item \textbf{Shared Memory} - Memory shared between cores on an SM. Slower than register access, but faster than accessing main memory.
\end{itemize}

\section{Code theory}

\subsection{Transfer time}

By measuring the round-trip transfer time by using CUDA events, and timing the total program runtime, I measured the percentage of total runtime used to transfer data between host and device to be $\sim0.1\%$
This is not a high enough percentage that I see a need to improve it. I'm also unsure as to how one would go about reducing transfer time in this case. The transfers can not be coalesced, and the amount of data as well as bandwidth is constant. On most machines, the difference between using \texttt{cudaMemcpy} and \texttt{cudaMemcpyAsync} will be minimal as both will delegate the work of actually transferring to the device to DMA. As far as I can tell, there are no obvious ways of decreasing transfer time.
%----------------------------------------------------------------------------------------

\end{document}

